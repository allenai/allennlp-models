{
    "id": "pair-classification-roberta-mnli",
    "registered_model_name": "basic_classifier",
    "registered_predictor_name": "textual_entailment",
    "display_name": "RoBERTa MNLI",
    "task_id": "textual_entailment",
    "archive_file": "mnli-roberta-2020-07-29.tar.gz",
    "model_details": {
        "description": "This `Model` implements a basic text classifier. The text is embedded into a text field
                        using a RoBERTa-large model. The resulting sequence is pooled using a cls_pooler
                        `Seq2VecEncoder` and then passed to a linear classification layer, which projects
                        into the label space.",
        "short_description": "RoBERTa finetuned on MNLI.",
        "developed_by": "Liu et al",
        "contributed_by": "Dirk Groeneveld",
        "date": "2020-07-29",
        "version": "1",
        "model_type": "RoBERTa",
        "paper": "[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://api.semanticscholar.org/CorpusID:198953378)",
        "citation": "@article{Liu2019RoBERTaAR,
                     title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
                     author={Y. Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
                     journal={ArXiv},
                     year={2019},
                     volume={abs/1907.11692}}",
        "license": null,
        "contact": "allennlp-contact@allenai.org",
        "training_config": "snli_roberta.jsonnet",
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "Accuracy",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": "[Multi-genre Natural Language Inference (MultiNLI)](https://cims.nyu.edu/~sbowman/multinli/) dev set",
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": "[Multi-genre Natural Language Inference (MultiNLI)](https://cims.nyu.edu/~sbowman/multinli/) train set",
        "motivation": null,
        "preprocessing": null
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "ethical_considerations": {
        "ethical_considerations": null
    },
    "caveats_and_recommendations": {
        "caveats_and_recommendations": null
    }
}
