{
    "id": "lm-next-token-lm-gpt2",
    "registered_model_name": "next_token_lm",
    "registered_predictor_name": null,
    "display_name": "GPT2-based Next Token Language Model",
    "task_id": "language-modeling",
    "model_details": {
        "description": "This is the public 345M parameter OpenAI GPT-2 language model for generating sentences. The model embeds some input tokens, contextualizes them, then predicts the next word, computing a loss against known target. \nIf `BeamSearch` is given, this model will predict a sequence of next tokens.",
        "short_description": "OpenAI's GPT-2 language model that generates the next token.",
        "developed_by": "Radford et al",
        "contributed_by": null,
        "date": "2020-06-30",
        "version": "1",
        "model_type": "GPT2",
        "paper": {
            "citation": "\n@inproceedings{Radford2019LanguageMA,\ntitle={Language Models are Unsupervised Multitask Learners},\nauthor={A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},\nyear={2019}}\n",
            "title": "Language Models are Unsupervised Multitask Learners",
            "url": "https://api.semanticscholar.org/CorpusID:160025533"
        },
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "Perplexity",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": {
            "name": "WebText corpus",
            "url": "https://github.com/openai/gpt-2-output-dataset"
        },
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": {
            "name": "WebText corpus",
            "url": "https://github.com/openai/gpt-2-output-dataset"
        },
        "motivation": "WebText emphasizes document quality. Only human-curated/filtered documents are scraped. Reddit outbound links which receive at least 3 karma points are taken as a proxy for human filtered webpages that are interesting.",
        "preprocessing": "Dragnet and [Newspaper](https://github.com/codelucas/newspaper) content extractors are used. Wikipedia articles are removed."
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "model_caveats_and_recommendations": {
        "caveats_and_recommendations": null
    },
    "model_ethical_considerations": {
        "ethical_considerations": null
    },
    "model_usage": {
        "archive_file": "gpt2-next-word-lm-2020.06.30.tar.gz",
        "training_config": null,
        "install_instructions": "pip install allennlp==1.0.0 allennlp-models==1.0.0"
    }
}