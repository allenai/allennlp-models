{
    "id": "pair-classification-decomposable-attention-elmo",
    "registered_model_name": "decomposable_attention",
    "registered_predictor_name": null,
    "display_name": "ELMo-based Decomposable Attention",
    "task_id": "textual_entailment",
    "model_details": {
        "description": "This `Model` implements the Decomposable Attention model described in [A Decomposable Attention Model for Natural Language Inference](https://api.semanticscholar.org/CorpusID:8495258) by Parikh et al., 2016, with some optional enhancements before the decomposable attention actually happens.  Parikh's original model allowed for computing an \"intra-sentence\" attention before doing the decomposable entailment step.  We generalize this to any `Seq2SeqEncoder` that can be applied to the premise and/or the hypothesis before computing entailment.\n\nThe basic outline of this model is to get an embedded representation of each word in thepremise and hypothesis, align words between the two, compare the aligned phrases, and make a final entailment decision based on this aggregated comparison.  Each step in this process uses a feedforward network to modify the representation.\n\nThis model uses ELMo embeddings.",
        "short_description": "The decomposable attention model (Parikh et al, 2017) combined with ELMo embeddings trained on SNLI.",
        "developed_by": "Parikh et al",
        "contributed_by": "Dirk Groeneveld",
        "date": "2020-04-09",
        "version": "1",
        "model_type": "Seq2Seq",
        "paper": {
            "citation": "\n@article{Parikh2016ADA,\ntitle={A Decomposable Attention Model for Natural Language Inference},\nauthor={Ankur P. Parikh and Oscar T{\"a}ckstr{\"o}m and Dipanjan Das and Jakob Uszkoreit},\njournal={ArXiv},\nyear={2016},\nvolume={abs/1606.01933}}\n",
            "title": "A Decomposable Attention Model for Natural Language Inference",
            "url": "https://api.semanticscholar.org/CorpusID:8495258"
        },
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "Accuracy",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": {
            "name": "Stanford Natural Language Inference (SNLI) dev set",
            "url": "https://nlp.stanford.edu/projects/snli/",
            "processed_url": "https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl"
        },
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": {
            "name": "Stanford Natural Language Inference (SNLI) train set",
            "url": "https://nlp.stanford.edu/projects/snli/",
            "processed_url": "https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl"
        },
        "motivation": null,
        "preprocessing": null
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "model_caveats_and_recommendations": {
        "caveats_and_recommendations": null
    },
    "model_ethical_considerations": {
        "ethical_considerations": null
    },
    "model_usage": {
        "archive_file": "decomposable-attention-elmo-2020.04.09.tar.gz",
        "training_config": "decomposable_attention_elmo.jsonnet",
        "install_instructions": "pip install allennlp==2.1.0 allennlp-models==2.1.0"
    }
}
