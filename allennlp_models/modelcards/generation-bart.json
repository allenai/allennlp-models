{
    "id": "generation-bart",
    "registered_model_name": "bart",
    "registered_predictor_name": null,
    "display_name": "BART",
    "archive_file": "bart-2020.07.25.tar.gz",
    "model_details": {
        "description": "The BART model here uses a language modeling head, and therefore can be used for generation.
                        The BART encoder, implemented as a `Seq2SeqEncoder`, which assumes it operates on
                        already embedded inputs.  This means that we remove the token and position embeddings
                        from BART in this module.  For the typical use case of using BART to encode inputs to your
                        model (where we include the token and position embeddings from BART), you should use
                        `PretrainedTransformerEmbedder(bart_model_name, sub_module=\"encoder\")` instead of this.",
        "short_description": "BART with a language model head for generation.",
        "developed_by": "Lewis et al",
        "contributed_by": "Dirk Groeneveld",
        "date": "2020-07-25",
        "version": "1",
        "model_type": "BART",
        "paper": "[BART: Denosing Sequence-to-Sequence Pre-training for Natural Language Generation,
                   Translation, and Comprehension](https://api.semanticscholar.org/CorpusID:204960716)",
        "citation": "@inproceedings{Lewis2020BARTDS,
                     title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
                     author={M. Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and A. Mohamed and Omer Levy and Ves Stoyanov and L. Zettlemoyer},
                     booktitle={ACL},
                     year={2020}}",
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "ROUGE and BLEU",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": "[CNN/DailyMail](https://github.com/abisee/cnn-dailymail)",
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": "[CNN/DailyMail](https://github.com/abisee/cnn-dailymail)",
        "motivation": null,
        "preprocessing": null
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "ethical_considerations": {
        "ethical_considerations": null
    },
    "caveats_and_recommendations": {
        "caveats_and_recommendations": null
    }
}
