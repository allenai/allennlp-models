{
    "id": "lm-masked-language-model",
    "registered_model_name": "masked_language_model",
    "registered_predictor_name": null,
    "display_name": "BERT-based Masked Language Model",
    "task_id": "masked-language-modeling",
    "model_details": {
        "description": "The `MaskedLanguageModel` embeds some input tokens (including some which are masked), contextualizes them, then predicts targets for the masked tokens, computing a loss against known targets.",
        "short_description": "BERT-based masked language model",
        "developed_by": "Devlin et al",
        "contributed_by": null,
        "date": "2020-10-07",
        "version": "1",
        "model_type": "BERT",
        "paper": {
            "citation": "\n@inproceedings{Devlin2019BERTPO,\ntitle={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\nauthor={J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\nbooktitle={NAACL-HLT},\nyear={2019}}\n",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "url": "https://api.semanticscholar.org/CorpusID:52967399"
        },
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "Perplexity",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": "BooksCorpus (800M words) and English Wikipedia (2,500M words).",
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": "BooksCorpus (800M words) and English Wikipedia (2,500M words).",
        "motivation": "Document-level corpus is used rather than shuffled sentence-level corpus, to extract long contiguous sequences.",
        "preprocessing": "For Wikipedia, text passages are extracted and lists, tables, and headers are ignored."
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "model_caveats_and_recommendations": {
        "caveats_and_recommendations": "NOTE: This was developed for use in a demo, not for training.  It's possible that it will still work for training a masked LM, but it is very likely that some other code would be much more efficient for that.  This `does` compute correct gradients of the loss, because we use that in our demo, so in principle it should be able to train a model, we just don't necessarily endorse that use."
    },
    "model_ethical_considerations": {
        "ethical_considerations": "BERT demonstrates gender bias in that it thinks the doctor is more likely a man ('his') than a woman ('her'). An important issue in NLP is how to understand and address such biases in our linguistic models."
    },
    "model_usage": {
        "archive_file": "bert-masked-lm-2020-10-07.tar.gz",
        "training_config": "lm/bidirectional_language_model.jsonnet",
        "install_instructions": "pip install allennlp==2.1.0 allennlp-models==2.1.0"
    }
}
