{
    "id": "rc-transformer-qa",
    "registered_model_name": "transformer_qa",
    "registered_predictor_name": null,
    "display_name": "Transformer QA",
    "task_id": "rc",
    "model_details": {
        "description": "The model implements a reading comprehension model patterned after the proposed model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)](https://api.semanticscholar.org/CorpusID:52967399), with improvements borrowed from the SQuAD model in the transformers project. It predicts start tokens and end tokens with a linear layer on top of word piece embeddings.",
        "short_description": "A reading comprehension model patterned after the proposed model in Devlin et al, with improvements borrowed from the SQuAD model in the transformers project",
        "developed_by": "Devlin et al",
        "contributed_by": "Dirk Groeneveld and Evan Pete Walsh",
        "date": "2020-10-03",
        "version": "2",
        "model_type": "RoBERTa",
        "paper": {
            "citation": "\n@article{Liu2019RoBERTaAR,\ntitle={RoBERTa: A Robustly Optimized BERT Pretraining Approach},\nauthor={Y. Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and L. Zettlemoyer and V. Stoyanov},\njournal={ArXiv},\nyear={2019},\nvolume={abs/1907.11692}}\n",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "url": "https://api.semanticscholar.org/CorpusID:198953378"
        },
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "F1-score, Span Accuracy, Exact Match",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": {
            "name": "SQuAD dev set",
            "url": "https://rajpurkar.github.io/SQuAD-explorer/explore/2.0/dev/",
            "processed_url": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/squad/squad-dev-v2.0.json"
        },
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": {
            "name": "SQuAD training set",
            "url": "https://rajpurkar.github.io/SQuAD-explorer/explore/2.0/dev/",
            "processed_url": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/squad/squad-train-v2.0.json"
        },
        "motivation": "For the pretrained RoBERTa model, document-level corpora were used rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences",
        "preprocessing": "For the pretrained RoBERTa model, only the text passages were extracted from English Wikipedia; lists, tables, and headers were ignored."
    },
    "quantitative_analyses": {
        "unitary_results": "On the validation set:\nF1: 88%\nExact match: 84%\nThese are metrics using the official evaluation. Note that the metrics that the model produces while training are calculated on a per-instance basis only. Since there could be more than one instance per question, these metrics are not the official numbers on the SQuAD task. To get official numbers, run the evaluation script at allennlp_models/rc/tools/transformer_qa_eval.py.",
        "intersectional_results": null
    },
    "model_caveats_and_recommendations": {
        "caveats_and_recommendations": null
    },
    "model_ethical_considerations": {
        "ethical_considerations": null
    },
    "model_usage": {
        "archive_file": "transformer-qa.2021-02-11.tar.gz",
        "training_config": "rc/transformer_qa.jsonnet",
        "install_instructions": "pip install allennlp==2.1.0 allennlp-models==2.1.0"
    }
}
