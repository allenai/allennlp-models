{
    "id": "rc-transformer-qa",
    "registered_model_name": "transformer_qa",
    "registered_predictor_name": null,
    "display_name": "Transformer QA",
    "archive_file": "transformer-qa-2020-05-26.tar.gz",
    "model_details": {
        "description": "The model implements a reading comprehension model patterned after the proposed model in 
                        https://api.semanticscholar.org/CorpusID:52967399 (Devlin et al, 2018), with improvements 
                        borrowed from the SQuAD model in the transformers project. It predicts start tokens and end
                        tokens with a linear layer on top of word piece embeddings.",
        "developed_by": "Devlin et al",
        "contributed_by": "Dirk Groeneveld",
        "date": "2020-05-26",
        "version": "1",
        "model_type": "Deep Birectional Transformers (BERT)",
        "paper": "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]
                  (https://api.semanticscholar.org/CorpusID:52967399)",
        "citation": "@inproceedings{Devlin2019BERTPO,
                     title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
                     author={J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
                     booktitle={NAACL-HLT},
                     year={2019}}",
        "license": null,
        "contact": "allennlp-contact@allenai.org"
    },
    "intended_use": {
        "primary_uses": null,
        "primary_users": null,
        "out_of_scope_use_cases": null
    },
    "factors": {
        "relevant_factors": null,
        "evaluation_factors": null
    },
    "metrics": {
        "model_performance_measures": "F1-score, Span Accuracy, Exact Match. Note that the metrics 
                                       that the model produces are calculated on a per-instance basis only.
                                       Since there could be more than one instance per question, these
                                       metrics are not the official numbers on the SQuAD task. To get
                                       official numbers, run the script in scripts/transformer_qa_eval.py.",
        "decision_thresholds": null,
        "variation_approaches": null
    },
    "evaluation_data": {
        "dataset": "The TransformerQA was evaluated on the SQuAD dev set.",
        "motivation": null,
        "preprocessing": null
    },
    "training_data": {
        "dataset": "The TransformerQA model was fine-tuned on SQuAD train set. 
                    The underlying BERT model was pretrained on BooksCorpus (800M words) (Zhu et al.,2015) 
                    and English Wikipedia (2,500M words).",
        "motivation": "For the pretrained BERT model, document-level corpora were used rather than a shuffled sentence-level corpus 
                       such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences",
        "preprocessing": "For the pretrained BERT model, only the text passages were extracted from English Wikipedia; 
                          lists, tables, and headers were ignored.",
    },
    "quantitative_analyses": {
        "unitary_results": null,
        "intersectional_results": null
    },
    "ethical_considerations": {
        "ethical_considerations": null
    },
    "caveats_and_recommendations": {
        "caveats_and_recommendations": null
    }
}
